
import traceback;
import tokenizer;
from tokenizer import Tokenizer;


class Translator(object):
    def __init__(self, text):
        self.text = text;
        self.t = Tokenizer(text);
        self.tokens = self.t.tokenize();
        self.pushbacks = [];
        self.inf_loop = False;
        self.block_depth = 0;
        self.need_newline = False;
        self.block_type = [];
    #;
    def indent_token(self, depth=None):
        if depth is None:
            depth = self.block_depth;
        #;
        token = self.t.IndentToken();
        token.text = ' ' * (4 * depth);
        return token;
    #;
    def look_token(self):
        token = self.pop_token();
        self.push_token(token);
        return token;
    #;
    def pop_token(self, acceptable=None):
        if self.pushbacks:
            res = self.pushbacks.pop();
        else:
            try:
                res = self.tokens.next();
            except StopIteration:
                return self.t.EOFToken();
            #;
        #;
        if acceptable is not None:
            if acceptable != type(res) and acceptable != res.text:
                raise Exception('Syntax Error');
            #;
        #;
        return res;
    #;
    def push_token(self, token):
        self.pushbacks.append(token);
    #;
    def translate(self):
        text = '';
        for token in self.translate_file():
            text += token.text;
        #;
        return text;
    #;
    def translate_file(self):
        try:
            while type(self.look_token()) != self.t.EOFToken:
                line = self.translate_line();
                if line:
                    for token in line:
                        yield token;
                    #;
                #;
            #;
        except Exception as e:
            traceback.print_exc();
        #;
    #;
    def translate_line(self):
        if type(self.look_token()) in [self.t.IndentToken, self.t.WhitespaceToken]:
            self.pop_token();
        #;
        if type(self.look_token()) == self.t.BlockEndianToken:
            self.pop_token();
            self.block_depth -= 1;
            self.need_newline = self.block_type.pop();
            return;
        #;
        tokens = [];
        while not type(self.look_token()) in [self.t.EOFToken, self.t.EOLToken]:
            tokens.append(self.pop_token());
        #;
        noeffect = True;
        for token in tokens:
            if type(token) in [self.t.EOFToken, self.t.EOLToken]:
                continue;
            #;
            if type(token) == self.t.CommentToken:
                continue;
            #;
            noeffect = False;
            break;
        #;
        if noeffect:
            self.pop_token();
            for token in tokens:
                yield token;
            #;
            return;
        #;

        if self.need_newline:
            eol = self.t.EOLToken();
            eol.text = '\n';
            yield eol;
            self.need_newline = False;
        #;
        yield self.indent_token();

        tokens.reverse();
        for token in tokens:
            self.push_token(token);
        #;
        eol = self.t.EOLToken();
        eol.text = '\n';
        yield eol;

        token = self.look_token();
        if token.text in tokenizer.CONTROL_KEYWORDS:
            depth = self.block_depth;
            if token.text in ['else', 'elif', 'except', 'finally']:
                depth -= 1;
            #;
            yield self.indent_token(depth);
            for t in self.translate_controlblock():
                yield t;
            #;
            return;
        #;
        yield self.indent_token();
        if token.text in ['from', 'import']:
            for t in self.translate_import():
                yield t;
            #;
            return;
        #;
        if token.text in tokenizer.DEFINITION_KEYWORDS:
            for t in self.translate_definition():
                yield t;
            #;
            return;
        #;
        if token.text == '@':
            for t in self.translate_annotation():
                yield t;
            #;
            return;
        #;
        for t in self.translate_to_endian():
            yield t;
        #;
        self.need_newline = False;
    #;
    def translate_import(self):
        if self.look_token().text == 'from':
            while not self.look_token().text == 'import':
                yield self.pop_token();
            #;
        #;
        if self.look_token().text == 'import':
            for token in self.translate_to_endian():
                yield token;
            #;
            self.need_newline = False;
        #;
    #;
    def translate_to_endian(self, endian=None, accept_endian=False):
        if endian is None:
            endian = [self.t.SemicolonToken];
        #;
        last_token = None;
        while not type(self.look_token()) in endian + [self.t.EOFToken]:
            if type(self.look_token()) == self.t.EOLToken:
                if last_token and last_token.text != '\\':
                    token = self.t.BackslashToken();
                    token.text = '\\';
                    yield token;
                #;
                yield self.pop_token();
                if type(self.look_token()) == self.t.IndentToken:
                    self.pop_token();
                #;
                yield self.indent_token(self.block_depth + 1);
                continue;
            #;
            last_token = self.pop_token();
            yield last_token;
        #;
        endian_token = self.pop_token();
        if type(endian_token) == self.t.EOLToken:
            return;
        #;
        if accept_endian:
            yield endian_token;
        #;
        if type(self.look_token()) == self.t.WhitespaceToken:
            yield self.pop_token();
        #;
        if type(self.look_token()) == self.t.CommentToken:
            yield self.pop_token();
        #;
        if type(self.look_token()) == self.t.EOLToken:
            self.pop_token();
        #;
    #;
    def translate_definition(self):
        if not self.look_token().text in tokenizer.DEFINITION_KEYWORDS:
            raise Exception('wrong call' + str(self.look_token()));
        #;
        self.block_depth += 1;
        self.block_type.append(True);
        for token in self.translate_to_endian([self.t.ColonToken], True):
            yield token;
        #;
    #;
    def translate_controlblock(self):
        if not self.look_token().text in tokenizer.CONTROL_KEYWORDS:
            raise Exception('wrong call' + str(self.look_token()));
        #;
        if not self.look_token().text in ['elif', 'else', 'except', 'finally']:
            self.block_depth += 1;
            self.block_type.append(False);
        #;
        for token in self.translate_to_endian([self.t.ColonToken], True):
            yield token;
        #;
    #;
    def translate_annotation(self):
        if self.look_token().text != '@':
            raise Exception('wrong call' + str(self.look_token()));
        #;
        for token in self.translate_to_endian([self.t.SemicolonToken, self.t.EOLToken]):
            yield token;
        #;
    #;
#;
